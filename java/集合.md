## fail-fast和fail-safe 在集合方面的体现
主要体现在迭代器方面，线程不安全的集合类都是fail-fast；JUC下的集合类都是fail-safe  
- fail-fast机制：在遍历时（用Iterator），如果集合对象的结构被修改了，就会抛出ConcurrentModificationException异常。例如你用迭代器在遍历一个list，这时候你调用list.remove会抛异常  
```
public static void failFast() {

        List<String> list = new ArrayList<>();

        list.add("item-1");
        list.add("item-2");
        list.add("item-3");
        list.add("item-4");

        Iterator<String> it = list.iterator();

        while (it.hasNext()) {
            String item = it.next();
            System.out.println(item);
            list.add("itme-5"); // 下次迭代时会抛出ConcurrentModificationException异常
        }

    }
```
- fail-safe机制：在遍历时（用Iterator），迭代器内部会复制当前数组。由于有写时复制，遍历过程中新增删除，会复制成一个新的数组，所以不会影响迭代。所以这儿体现的是新增删除即使异常了，也不会对迭代器造成影响，也就是fail-safe思想
```
public static void failSafe() {

        List<String> list = new CopyOnWriteArrayList<>();

        list.add("item-1");
        list.add("item-2");
        list.add("item-3");
        list.add("item-4");

        Iterator<String> it = list.iterator();

        while (it.hasNext()) {
            String item = it.next();
            System.out.println(item);
            list.add("itme-5");
        }

        System.out.println(list.size()); // 会打印出来8，迭代四次，四个新元素插入到了集合中。
    }
```

## Copy-On-Write  
   - 简称 COW，在往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器
   - CopyOnWrite 容器的 add/remove 等写方法是需要加锁的，目的是为了避免 Copy 出 N 个副本出来，导致并发写。读方法不加锁（为了提高并发性能），读到的数据可能不是最新的，属于弱一致性

## List
### ArrayList 
是一个可改变大小的数组，当更多的元素加入到 ArrayList 中时，其大小将会动态地增长，内部的元素可以直接通过 get 与 set 方法进行访问，add 和 remove 的性能弱，get 和 set 性能强，默认情况下 ArrayList 的初始容量非常小,所以如果可以预估数据量的话,分配一个较大的初始值属于最佳实践,这样可以减少调整大小的开销
- **初始大小为0，在第一次add时候会进行扩容，1.5倍扩容**
- add可以指定插入位置，set是覆盖指定位置。
- **线程不安全原因：**   
    1、add时候会size++，并发时会产生数据覆盖以及size不准  
    2、线程可见性，数组可见性问题，即使加了volatile也不行，因为加了只是对数组可见，但是元素还是不行
- 动态删除、新增：直接在for循环里add、remove会报错，要用迭代器
- fail-fast: 当对ArrayList做添加或者删除元素的操作时，都会修改modCount这个变量，而ArrayList的迭代器每次迭代的时候，又都回去检查当前modCount和迭代器产生时的expectedModCount变量是否相等，如果不等就会抛出ConcurrentModificationException异常

### LinkedList 
是一个**双向链表**，因为是链表所以不需要**扩容**。add 和 remove 的性能强，get 和 set 性能弱。LinkedList 还实现了 **Queue和Deque（可当做栈使用）** 接口，该接口比 List 提供了更多的方法，包括 offer(),peek(),poll()等.

### Vector 
跟 ArrayList 的实现类似，但是所有读写方法都实现了 synchronized。Vector 2倍扩容，而 ArrayList 1.5倍扩容

### CopyOnWriteArrayList 
- add、remove 方法使用 lock 加了锁，当有新元素 add/set/remove 时，先从原有的数组中拷贝一份出来，然后在新的数组做写操作，写完之后，再将原来的数组引用指向到新数组
- 迭代器支持 hasNext(), next()等不可变操作，但不支持可变remove()、set(xx)、add(xx)等操作。注意这里是迭代器的方法。
- 动态删除、新增：因为有写时复制，所以直接在for循环里add、remove即可。

### Collections.synchronizedList 和 Vector 的区别
- Vector 使用同步方法实现，synchronizedList 使用同步代码块实现
- SynchronizedList 有很好的扩展和兼容功能。他可以将所有的 List 的子类转成线程安全的类
- SynchronizedList 在 listIterator 的时候没有枷锁，所以进行遍历时要手动进行同步处理
- SynchronizedList 可以指定锁定的对象  

### Arrays.asList
- Arrays.asList 得到的只是一个 Arrays 的内部类，一个原来数组的视图 List，因此如果对它进行增删操作会报错，只能遍历，要转成其他 List 才能增删（例如 new ArrayList()）

### 如何在遍历的同时删除 ArrayList 中的元素
- 使用普通 for 循环删除（需要调整循环的下标）
- 直接使用 Iterator 的 remove 进行删除
- 使用 jdk8 中提供的 filter 过滤
- 使用 jdk8 中的 removeIf 方法
- 使用 fail-safe 的集合类（例如：ConcurrentLinkedDeque）

---

## Set
### HashSet
- 基于 HashMap 实现，只用了key，value为一个静态object对象。利用 HashMap 同一个 Hash 值只能有一个 Value 的特性来实现 Set 的去重功能

### LinkedHashSet
- 基于 LinkedHashMap 实现，利用 LinkedHashMap 同一个 Hash 值只能有一个 Value 的特性来实现 Set 的去重功能

### TreeSet
- 基于 TreeMap 实现，利用 TreeMap 同一个 Hash 值只能有一个 Value 的特性来实现 Set 的去重功能

### HashSet 和 TreeSet 的区别
- TreeSet 是红黑树实现的，Treeet 中的数据是自动排好序的，不允许放入 null 值
- HashSet 是哈希表实现的，HashSet 中的数据是无序的，可以放入 null，但只能放入一个 null

### Set 如何保证元素不重复
- 当向 HashSet 中添加元素的时候，首先计算元素的 hashcode 值，然后通过扰动计算和按位与的方式计算出这个元素的存储位置，如果这个位置位空，就将元素添加进去；如果不为空，则用 equals 方法比较元素是否相等，相等就不添加，否则找一个空位添加
- TreeSet 是用元素的 compareTo()方法来判断重复元素的

### Set 和 List 区别
- 都继承自 Collection
- 都是用来存储一组相同类型的元素
- List 中元素有放入顺序，元素可重复
- Set 中元素无放入顺序，元素不可重复

## Map
### HashMap
[介绍地址](https://www.pdai.tech/md/java/collection/java-map-HashMap&HashSet.html)  

**JDK1.7 及 1.7 之前的版本（数组+链表）**  
- HashMap 实现了 Map 接口，即允许放入 key 为 null 的元素，也允许插入 value 为 null 的元素
- 采用数组+链表的方式存储数据
- 存储的对象必须实现 hashCode()和 equals()两个方法
- 写入数据时，先根据对象的 hashCode 获取数组的下标，再获取到数组中对应位置的链表（解决了 hashcode 冲突的问题，也成为拉链法），然后插入数据到链表中，equals 方法决定对象存入链表时是覆盖还是新增
- 读取数据时，先根据对象的 hashCode 获取数组的下标，再获取到数组中对应位置的链表，然后依次遍历链表，通过 equals 方法判断是否命中

**JDK1.8 及 1.8 之后的版本（数组+链表+红黑树）**  
- 采用数组+链表+红黑树的方式存储数据
- 为什么要用红黑树：二叉查找树会退化，但是红黑树会通过左旋右旋避免
- 为了防止链表过长，当数组的长度大于 64 且链表的长度大于 8 的时候会将链表转换成红黑树（使用头结点的值来判断是否是链表，大于 0 表示是链表，如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树）
- 读取数据时，会根据头结点判断对应数组下标处的是链表还是红黑树，如果是链表，则遍历得到结果，如果是红黑树，执行查找得到结果
- 扩容机制：与 JDK1.7 的区别是，JDK7 是先扩容后插入新值的，JDK8 先插入新值再扩容

**扩容机制**  
JDK7使用是hash值与数组长度-1 这个掩码进行与运算，得到Entry元素的新下标位置，得到的结果直接就是下标位置 ；  
Jdk1.8中是使用hashcode与 数组长度 进行与运算，得到的是0 或者非零。如果是0
表示新位置下标不变，如果不是0那么表示位置有变动，如果有变动下标位置是原位置加上原数组长度。

**线程不安全体现**  
1. 同时put，如果多个线程同时定位到一个链表中的同一个index，则会导致只有一个数据插入链表成功
2. 并发扩容导致：当多个线程同时检测到总数量超过门限值的时候就会同时调用resize操作，各自生成新的数组并rehash后赋给该map底层的数组table，结果最终只有最后一个线程生成的新数组被赋给table变量，其他线程的均会丢失。而且当某些线程已经完成赋值而其他线程刚开始的时候，就会用已经被赋值的table作为原始数组，这样也会有问题。

**为什么 capacity 数组容量必须为 2^n：**  
```
// hash 算法
// 对hashcode进行二次hash可以获得更好的散列
static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

// index n是capacity
// 直接取模的话性能比较差。（n-1）& 相当于取模，但是有个前提必须是2^n才可以。
index = (n - 1) & hash
```

**为什么负载因子的值默认为 0.75：**  
加载因子是表示 Hash 表中元素的填满的程度，加载因子越大，填满的元素越多，空间利用率越高，但冲突的机会加大，反之加载因子越小，填满的元素越少，冲突的机会减小，但空间浪费多。冲突的机会越大，则查找的成本越高，反之，查找的成本越小，因此，必须在 "hash 冲突率"与"空间利用率"之间寻找一种平衡与折中，所以默认采用 0.75  

## Hashtable 
是 JDK1.0 新增的，是遗留类，项目中不应该去使用它，实现原理和 HashMap 类似，也是通过数组+链表的方式实现，但它是线程安全的，因为所有读写方法都实现了 synchronized 锁

## LinkedHashMap
- LinkedHashMap 是 HashMap 的直接子类，二者唯一的区别是 LinkedHashMap 在 HashMap 的基础上，在 Entry 数组之外，采用双向链表的形式将所有 entry 连接起来，这样是为保证元素的迭代顺序跟插入顺序相同，**该双向链表的迭代顺序就是 entry 的插入顺序**
- 写入数据时，会先执行一次查询，数据如果已存在则直接返回，数据不存在时，按照 HashMap 的方式将数据插入到对应链表中，并将数据插入到双向链表的**尾部**
- 迭代 LinkedHashMap 时，不需要像 HashMap 那样遍历整个 table，而只需要直接遍历 header 指向的双向链表即可，也就是说 LinkedHashMap 的迭代时间就只跟 entry 的个数相关，而跟 table 的大小无关，所以当 Map 的数据量不大，而容量很大时，LinkedHashMap 的遍历速度比 HashMap 快
- LinkedHashMap 提供了 removeEldestEntry 方法，该方法的作用是告诉 Map 是否要删除最早插入的 Entry，如果该方法返回 True，那么最早插入的元素会被删除，在每次插入新元素的之后 LinkedHashMap 会自动询问 removeEldestEntry()是否要删除最老的元素，**因此可以通过重载该方法，来实现 LRU 算法（固定大小的 FIFO 策略的缓存）**
    - 要实现lru要做2件事，1、accessOrder设置成true（默认是false， false基于插入顺序 true 基于访问顺序）2、重写removeEldestEntry（返回true删除，false不删除）
```
class LRULinkedHashMap<K,V> extends LinkedHashMap<K,V>{  
    //定义缓存的容量  
    private int capacity;  
    private static final long serialVersionUID = 1L;  
    //带参数的构造器     
    LRULinkedHashMap(int capacity){  
        //调用LinkedHashMap的构造器，传入以下参数  
        super(16,0.75f,true);  
        //传入指定的缓存最大容量  
        this.capacity=capacity;  
    }  
    //实现LRU的关键方法，如果map里面的元素个数大于了缓存最大容量，则删除链表的顶端元素  
    @Override  
    public boolean removeEldestEntry(Map.Entry<K, V> eldest){   
        System.out.println(eldest.getKey() + "=" + eldest.getValue());    
        return size()>capacity;  
    }    
}  
```

## TreeMap
- TreeMap 底层通过红黑树(Red-Black tree)实现，也就意味着 containsKey(), get(), put(), remove()都有着 O(logn)的时间复杂度
- TreeMap 不允许放入 null 值
- TreeMap 中的数据是自动排好序的，TreeMap 会按 key 升序排序，元素在插入 TreeMap 时 compareTo()方法要被调用，所以 想自定义排序必须要实现 Comparable 接口

## ConcurrentHashMap
- JDK1.5~1.7 版本（分段锁）：
    - ConcurrentHashMap 维护了一个 Segment 数组，将整个 Hash 表划分为多个分段
    - Segment 继承了 ReentrantLock，实现了独占模式的可重入锁，为每一个 segment 提供了线程安全的保障
    - Segment 内部是由数组+链表组成，维护了哈希散列表的若干个桶，每个桶是由 HashEntry 构成的链表，Segment 内部的数组支持扩容
    - 写入数据之前需要先获取该 Segment 的独占锁，再执行类似 HashMap 的写入逻辑
    - 读取数据时，先根据 Key 计算 hash 值，找到对应的 sgment，再根据 hash 找到 sgment 内部的数组中的对应链表，最后使用 key.equals 方法遍历链表得到结果
    - 缺点：最大并发度受 Segment 的个数限制
    - 
- JDK1.8 及 1.8 之后的版本（数组+链表+红黑树）
    - 采用类似 JDK1.8 的 HashMap 的数据+链表+红黑树方式存储数据
    - put 时，先根据 hashcode 获取到数组下标对应的链表，如果数组对应下标的链表为空，使用 CAS 创建链表，如果链表已存在，使用 synchronized 对链表加锁，然后再插入数据
    - 为了防止链表过长，当数组的长度大于 64 且链表的长度大于 8 的时候会将链表转换成红黑树（跟 JDK8 的 HashMap 一样，使用头结点的值来判断是否是链表，大于 0 表示是链表，如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树）
    - 读取数据时，先根据 Key 计算 hash 值，找到数组中的对应链表，再根据头结点判断是链表或者红黑树或者正在扩容，如果是链表，则遍历得到结果，如果是红黑树，执行查找得到结果，如果正在扩容，会等待取到锁后再读取数据
    - 扩容机制：数组每次都是双倍扩容，将原来的 tab 数组的元素迁移到新的 nextTab 数组中，rehash方法就跟hashmap一样。如果有新的线程get数据，则取原数组的， put 数据，会先帮助完成扩容，再 put 数据（put、computifabsent等操作都会帮助扩容，这些操作都可能是并发的，所以concurrenthashmap也叫做多线程扩容，每个线程最少分到 16 个链表，各自处理，不会互相影响）

### ConcurrentHashMap的读是否要加锁，为什么
不需要，get操作可以无锁是由于Node的元素val和指针next是用volatile修饰的，在多线程环境下线程A修改结点的val或者新增节点的时候是对线程B可见的。get操作全程不需要加锁是因为Node的成员val是用volatile修饰的和数组用volatile修饰没有关系。

### ConcurrentHashMap的迭代器是强一致性的迭代器还是弱一致性的迭代器?
弱一致性的迭代器，在这种迭代方式中，当iterator被创建后集合再发生改变就不再是抛出ConcurrentModificationException，取而代之的是使用fail-safe，但是代价就是新数据不能被读到，也就是弱一致性的体现。

## ConcurrentSkipListMap
ConcurrentSkipListMap 是一个内部使用跳跃表，并且支持排序和并发的一个 Map（treemap安全版），是线程安全的

## HashMap 和 Hashtable 区别
1. 线程安全：Hashtable 是线程安全，HashMap 是非线程安全。HashMap 的性能会高于 Hashtable，Hashtable 的所有方法都实现了 synchronize 锁
2. 是否可以使用 null 作为 key：HashMap 允许将 null 作为一个 entry 的 key 或者 value，而 Hashtable 不允许
3. 继承和实现：HashTable 继承自 Dictionary 类，而 HashMap 是 Java1.2 引进的 Map interface 的一个实现
4. 计算 hash 的方法：Hashtable 计算 hash 是直接使用 key 的 hashcode 对 table 数组的长度直接进行取模，而 HashMap 计算 hash 是对 key 的 hashcode 进行了二次 hash，以获得更好的散列值，然后对 table 数组长度取模
5. 初始容量及扩容机制：HashMap 的初始容量为 16，Hashtable 初始容量为 11。HashMap 扩容时是当前容量翻倍即：capacity 2，Hashtable 扩容时是容量翻倍+1 即：capacity (2+1)
6. 遍历方式：都使用了 Iterator，但 Hashtable 还使用了 Enumeration 的方式 且用 Enumeration 时不支持 fail-fast（遍历过程中如果元素被修改会导致遍历失败，可以用 Iterator 的 remove 方法避免这种情况）

## HashMap 和 ConcurrentHashMap 的区别
- 都是使用桶数组（数组+链表）实现
- 1.7 之前的 ConcurrentHashMap 对桶数组进行了分段，并且在每一个分段上都用 ReentrantLock 锁进行了保护
- 1.8 之后的 ConcurrentHashMap 采用数组+链表+红黑树的方式实现，而加锁则采用 CAS 和 synchronized 实现
- ConcurrentHashMap 支持 fail-safe （遍历过程中如果元素被修改不会有任何影响），因为 ConcurrentHashMap 的迭代器是弱一致性